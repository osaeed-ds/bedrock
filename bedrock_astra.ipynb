{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osaeed-ds/bedrock/blob/main/bedrock_astra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6715bc2b",
      "metadata": {
        "id": "6715bc2b"
      },
      "source": [
        "# Vector Similarity Astra-Bedrock Search QA Quickstart\n",
        "\n",
        "Set up a simple Question-Answering system with LangChain and Amazon Bedrock, using Astra DB as the Vector Database."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761d9b70",
      "metadata": {
        "id": "761d9b70"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Make sure you have a vector-capable Astra database (get one for free at [astra.datastax.com](https://astra.datastax.com)):\n",
        "\n",
        "- You will be asked to provide the **Database ID** for your Astra DB instance (see [here](https://awesome-astra.github.io/docs/pages/astra/faq/#where-should-i-find-a-database-identifier) for details);\n",
        "- Ensure you have an **Access Token** for your database with role _Database Administrator_ (see [here](https://awesome-astra.github.io/docs/pages/astra/create-token/) for details).\n",
        "\n",
        "Likewise, you will need the credentials to your Amazon Web Services identity, with access to **Amazon Bedrock**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18548e31",
      "metadata": {
        "id": "18548e31"
      },
      "source": [
        "## Set up your Python environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "042f832e",
      "metadata": {
        "tags": [],
        "id": "042f832e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aaef733-7c53-4885-d145-6ce857a376e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m760.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet \\\n",
        "  \"cassio>=0.1.3\" \\\n",
        "  \"langchain==0.0.249\" \\\n",
        "  \"boto3==1.28.62\" \\\n",
        "  \"botocore==1.31.62\" \\\n",
        "  \"cohere==4.37\" \\\n",
        "  \"openai==1.3.7\" \\\n",
        "  \"tiktoken==0.5.2\" \\\n",
        "  \"awscli==1.29.62\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c840842",
      "metadata": {
        "id": "6c840842"
      },
      "source": [
        "## Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b243d1b4",
      "metadata": {
        "tags": [],
        "id": "b243d1b4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "import boto3\n",
        "import cassio\n",
        "\n",
        "from langchain.embeddings import BedrockEmbeddings\n",
        "from langchain.llms import Bedrock\n",
        "from langchain.vectorstores import Cassandra\n",
        "from langchain.schema import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fccce0c2",
      "metadata": {
        "id": "fccce0c2"
      },
      "source": [
        "## Astra DB Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b0a92b-f9ce-4810-a8ef-5741b2449b18",
      "metadata": {
        "tags": [],
        "id": "a9b0a92b-f9ce-4810-a8ef-5741b2449b18"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_ID = input(\"Enter your Astra DB ID ('0123abcd-'):\")\n",
        "ASTRA_DB_APPLICATION_TOKEN = getpass(\"Enter your Astra DB Token ('AstraCS:...'):\")\n",
        "ASTRA_DB_KEYSPACE = input(\"Enter your keyspace name (optional, default keyspace used if not provided):\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a066f378-8fdb-4d4b-a7b1-bf685fbfd413",
      "metadata": {
        "tags": [],
        "id": "a066f378-8fdb-4d4b-a7b1-bf685fbfd413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d654e97-0e92-4143-b10d-123582063d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 172fa27f-656f-4584-8a7e-174b794d3cf0-westus3.db.astra.datastax.com:29042:04e14236-6e3d-41d4-93b5-9c5e1beb16de. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 172fa27f-656f-4584-8a7e-174b794d3cf0-westus3.db.astra.datastax.com:29042:04e14236-6e3d-41d4-93b5-9c5e1beb16de. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(137154635900160) 172fa27f-656f-4584-8a7e-174b794d3cf0-westus3.db.astra.datastax.com:29042:04e14236-6e3d-41d4-93b5-9c5e1beb16de> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 172fa27f-656f-4584-8a7e-174b794d3cf0-westus3.db.astra.datastax.com:29042:04e14236-6e3d-41d4-93b5-9c5e1beb16de. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ],
      "source": [
        "cassio.init(\n",
        "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
        "    database_id=ASTRA_DB_ID,\n",
        "    keyspace=ASTRA_DB_KEYSPACE if ASTRA_DB_KEYSPACE else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c674f1",
      "metadata": {
        "id": "33c674f1"
      },
      "source": [
        "## AWS Credentials Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3219cb02-f955-4fb3-85af-3f149868958a",
      "metadata": {
        "id": "3219cb02-f955-4fb3-85af-3f149868958a"
      },
      "source": [
        "_Note_: in the following cells you will be asked to explicitly provide the credentials to your AWS account. These are set as environment variables for usage by the subsequent `boto3` calls. Please refer to [boto3's documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) on the possible ways to supply your credentials in a more production-like environment.\n",
        "\n",
        "In particular, if you are running this notebook in **Amazon SageMaker Studio**, please note that it is sufficient to add the Bedrock policy to your SageMaker role, as outlined at [this link](https://github.com/aws-samples/amazon-bedrock-workshop#enable-aws-iam-permissions-for-bedrock), to access the Bedrock services. In that case you can skip the following three setup cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccbb76ea",
      "metadata": {
        "tags": [],
        "id": "ccbb76ea"
      },
      "outputs": [],
      "source": [
        "# Input your AWS Access Key ID\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = getpass(\"Your AWS Access Key ID:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93c873d",
      "metadata": {
        "tags": [],
        "id": "f93c873d"
      },
      "outputs": [],
      "source": [
        "# Input your AWS Secret Access Key\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = getpass(\"Your AWS Secret Access Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "737249f5",
      "metadata": {
        "tags": [],
        "id": "737249f5"
      },
      "outputs": [],
      "source": [
        "# Input your AWS Session Token\n",
        "os.environ[\"AWS_SESSION_TOKEN\"] = getpass(\"Your AWS Session Token:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4388ac1d",
      "metadata": {
        "id": "4388ac1d"
      },
      "source": [
        "## Set up AWS Bedrock objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d65c46f0",
      "metadata": {
        "tags": [],
        "id": "d65c46f0"
      },
      "outputs": [],
      "source": [
        "bedrock_runtime = boto3.client(\"bedrock-runtime\", \"us-west-2\")\n",
        "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",\n",
        "                                       client=bedrock_runtime)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d9f48b",
      "metadata": {
        "id": "29d9f48b"
      },
      "source": [
        "## Set up the Vector Store\n",
        "\n",
        "This command will create a suitable table in your database if it does not exist yet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "29d9f48c",
      "metadata": {
        "tags": [],
        "id": "29d9f48c"
      },
      "outputs": [],
      "source": [
        "vector_store = Cassandra(\n",
        "    embedding=bedrock_embeddings,\n",
        "    table_name=\"shakespeare_act5\",\n",
        "    session=None,  # <-- meaning: use the global defaults from cassio.init()\n",
        "    keyspace=None,  # <-- meaning: use the global defaults from cassio.init()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af24199",
      "metadata": {
        "id": "6af24199"
      },
      "source": [
        "## Populate the database\n",
        "\n",
        "Add lines for the text of \"Romeo and Astra\", Scene 5, Act 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1dab5114",
      "metadata": {
        "tags": [],
        "id": "1dab5114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c242de-5cc0-4f2b-a0d3-7d4aa1e09a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75985  100 75985    0     0   224k      0 --:--:-- --:--:-- --:--:--  225k\n"
          ]
        }
      ],
      "source": [
        "# retrieve the text of a scene from act 5 of Romeo and Astra.\n",
        "# Juliet's name was changed to Astra to prevent the LLM from \"cheating\" when providing an answer.\n",
        "! mkdir -p \"texts\"\n",
        "! curl \"https://raw.githubusercontent.com/awesome-astra/docs/main/docs/pages/aiml/aws/bedrock_resources/romeo_astra.json\" \\\n",
        "    --output \"texts/romeo_astra.json\"\n",
        "input_lines = json.load(open(\"texts/romeo_astra.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb6b732",
      "metadata": {
        "id": "4cb6b732"
      },
      "source": [
        "Next, you'll populate the database with the lines from the play.\n",
        "This can take a couple of minutes, please be patient.  In total there are 321 lines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7bae5520",
      "metadata": {
        "tags": [],
        "id": "7bae5520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748f1320-0f27-42a3-9ea7-949c99243850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding 321 documents ... Done.\n"
          ]
        }
      ],
      "source": [
        "input_documents = []\n",
        "\n",
        "for input_line in input_lines:\n",
        "    if (input_line[\"ActSceneLine\"] != \"\"):\n",
        "        (act, scene, line) = input_line[\"ActSceneLine\"].split(\".\")\n",
        "        location = \"Act {}, Scene {}, Line {}\".format(act, scene, line)\n",
        "        metadata = {\"act\": act, \"scene\": scene, \"line\": line}\n",
        "    else:\n",
        "        location = \"\"\n",
        "        metadata = {}\n",
        "    quote_input = \"{} : {} : {}\".format(location, input_line[\"Player\"], input_line[\"PlayerLine\"])\n",
        "    input_document = Document(page_content=quote_input, metadata=metadata)\n",
        "    input_documents.append(input_document)\n",
        "\n",
        "print(f\"Adding {len(input_documents)} documents ... \", end=\"\")\n",
        "vector_store.add_documents(documents=input_documents, batch_size=50)\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d162c2d1-188e-43f0-b1c3-342b80641060",
      "metadata": {
        "id": "d162c2d1-188e-43f0-b1c3-342b80641060"
      },
      "source": [
        "## Answer questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "5332b838-6c1f-40f4-a29e-2b2d0250f408",
      "metadata": {
        "tags": [],
        "id": "5332b838-6c1f-40f4-a29e-2b2d0250f408"
      },
      "outputs": [],
      "source": [
        "prompt_template_str = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea67346b-d5ca-433d-858e-5c21397f9de5",
      "metadata": {
        "tags": [],
        "id": "ea67346b-d5ca-433d-858e-5c21397f9de5"
      },
      "source": [
        "We choose to use the following LLM model (see [this page](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html#model-parameters-general) for more info):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ee8b2ee0-f9bf-4ada-8fde-7d917d89c6fa",
      "metadata": {
        "tags": [],
        "id": "ee8b2ee0-f9bf-4ada-8fde-7d917d89c6fa"
      },
      "outputs": [],
      "source": [
        "model_id = \"anthropic.claude-v2\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4081fd78-1710-4a42-a942-a14f652c854d",
      "metadata": {
        "id": "4081fd78-1710-4a42-a942-a14f652c854d"
      },
      "source": [
        "Here the question-answering function is set up, implementing the RAG pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4e82ef49-ffec-4429-bcc2-ea09f50333cd",
      "metadata": {
        "tags": [],
        "id": "4e82ef49-ffec-4429-bcc2-ea09f50333cd"
      },
      "outputs": [],
      "source": [
        "req_accept = \"application/json\"\n",
        "req_content_type = \"application/json\"\n",
        "\n",
        "# This, created from the vector store, will fetch the top relevant documents given a text query\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "def answer_question(question: str, verbose: bool = False) -> str:\n",
        "    if verbose:\n",
        "        print(f\"\\n[answer_question] Question: {question}\")\n",
        "    # Retrieval of the most relevant stored documents from the vector store:\n",
        "    context_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join(doc.page_content for doc in context_docs)\n",
        "    if verbose:\n",
        "        print(\"\\n[answer_question] Context:\")\n",
        "        print(context)\n",
        "    # Filling the prompt template with the current values\n",
        "    llm_prompt_str = prompt.format(\n",
        "        question=question,\n",
        "        context=context,\n",
        "    )\n",
        "    # Invocation of the Amazon Bedrock LLM for text completion -- ultimately obtaining the answer\n",
        "    llm_body = json.dumps({\"prompt\": llm_prompt_str, \"max_tokens_to_sample\": 500})\n",
        "    llm_response = bedrock_runtime.invoke_model(\n",
        "        body=llm_body,\n",
        "        modelId=model_id,\n",
        "        accept=req_accept,\n",
        "        contentType=req_content_type,\n",
        "    )\n",
        "    llm_response_body = json.loads(llm_response[\"body\"].read())\n",
        "    answer = llm_response_body[\"completion\"].strip()\n",
        "    if verbose:\n",
        "        print(f\"\\n[answer_question] Answer: {answer}\\n\")\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "599d8856-921e-4bf4-8979-ff54b13de6d5",
      "metadata": {
        "tags": [],
        "id": "599d8856-921e-4bf4-8979-ff54b13de6d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c6c246-7888-4886-cadb-ebb043e9fba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Based on the provided context, it seems that Astra dies in the story. The lines mention Astra being found dead, warm, and newly killed, as well as characters reacting to and being affected by Astra's death.\n"
          ]
        }
      ],
      "source": [
        "my_answer = answer_question(\"Who dies in the story?\")\n",
        "print(\"=\" * 60)\n",
        "print(my_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db71cc88-61e3-42e5-802b-4ba4eaa795b4",
      "metadata": {
        "id": "db71cc88-61e3-42e5-802b-4ba4eaa795b4"
      },
      "source": [
        "Let's take a look at the RAG process piece-wise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6fbeaf2d-f589-4d04-8447-4b876375f5b1",
      "metadata": {
        "tags": [],
        "id": "6fbeaf2d-f589-4d04-8447-4b876375f5b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326c91a9-ee22-4d20-eb68-d0a424682baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[answer_question] Question: Who dies in the story?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[answer_question] Context:\n",
            "Act 5, Scene 3, Line 184 : First Watchman : And Astra bleeding, warm, and newly dead,\n",
            "Act 5, Scene 3, Line 244 : FRIAR LAURENCE : Was Tybalt's dooms-day, whose untimely death\n",
            "Act 5, Scene 3, Line 300 : PRINCE : Came to this vault to die, and lie with Astra.\n",
            "Act 5, Scene 3, Line 282 : BALTHASAR : I brought my master news of Astra's death,\n",
            "Act 5, Scene 3, Line 206 : First Watchman : Warm and new kill'd.\n",
            "\n",
            "[answer_question] Answer: Based on the context provided, it seems that Astra is the one who dies in the story. The lines mention Astra being found dead, warm, and newly killed, as well as characters reacting to and speaking about Astra's death.\n",
            "\n",
            "============================================================\n",
            "Based on the context provided, it seems that Astra is the one who dies in the story. The lines mention Astra being found dead, warm, and newly killed, as well as characters reacting to and speaking about Astra's death.\n"
          ]
        }
      ],
      "source": [
        "my_answer = answer_question(\"Who dies in the story?\", verbose=True)\n",
        "print(\"=\" * 60)\n",
        "print(my_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715ba03a-66d1-47fe-9a8e-6b2713ddd0f9",
      "metadata": {
        "id": "715ba03a-66d1-47fe-9a8e-6b2713ddd0f9"
      },
      "source": [
        "### Interactive QA session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9fe0df9c-b24d-4f35-aee4-8bef2dd1e1a6",
      "metadata": {
        "tags": [],
        "id": "9fe0df9c-b24d-4f35-aee4-8bef2dd1e1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "outputId": "f82d18f2-49ba-4d77-a922-1f4be46d3b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a question (empty to quit):what family does astra belong to?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer ==> Based on the context provided, it seems that Astra belongs to the Capulet family. The references to her being Romeo's love interest indicate she is playing the role of Juliet, who is a Capulet. \n",
            "\n",
            "In English: Astra belongs to the Capulet family.\n",
            "\n",
            "In Pig Latin: Astra elongsbay otay ethay Apuletcay amilyfay.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-da90550975e5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a question (empty to quit):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer ==> {answer_question(user_question)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "user_question = \"\"\n",
        "while True:\n",
        "    user_question = input(\"Enter a question (empty to quit):\").strip()\n",
        "    if user_question:\n",
        "        print(f\"Answer ==> {answer_question(user_question)}\")\n",
        "    else:\n",
        "        print(\"[User, AI exeunt]\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the prompt and see the difference in answer\n",
        "The existing prompt gives instructions not to guess at an answer and reply that the answer cannot be determined. The dataset was also rigged to replace Juliet with Astra so that it will not try to search on its own.  We are going to test with a new prompt that does the following.\n",
        "1. If an answer is not known, replace Astra with Juliet in the context\n",
        "2. Give the answer in both English and Pig Latin\n",
        "\n",
        "We will ask the same question with both prompts."
      ],
      "metadata": {
        "id": "xWZniWbLtMHM"
      },
      "id": "xWZniWbLtMHM"
    },
    {
      "cell_type": "code",
      "source": [
        "#existing prompt that does not try to guess and is unaware of Juliet\n",
        "prompt_template_str = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template_str)"
      ],
      "metadata": {
        "id": "vsHFjpvpukRW"
      },
      "id": "vsHFjpvpukRW",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_answer = answer_question(\"What family does Astra belong to?\")\n",
        "print(\"=\" * 60)\n",
        "print(my_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBJbuLNVt3zQ",
        "outputId": "5acf2869-90e7-4ee3-bea5-89e5269f3bcb"
      },
      "id": "rBJbuLNVt3zQ",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Based on the context provided, I do not have enough information to determine what family Astra belongs to. The lines mention Astra along with Romeo and Tybalt, who are from the Montague and Capulet families respectively, but there is no clear indication of which family Astra herself belongs to. Without more context about her relationships and background, I cannot confidently answer the question.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new prompt that is aware of Juliet and will also answer in Pig Latin\n",
        "prompt_template_str = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end.\n",
        "If you don't know the answer, replace any instances of Astra in the Context with Juliet, then attempt to answer.  give the answer in english and in pig latin.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template_str)"
      ],
      "metadata": {
        "id": "TNQvrzyitGZn"
      },
      "id": "TNQvrzyitGZn",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_answer = answer_question(\"What family does Astra belong to?\")\n",
        "print(\"=\" * 60)\n",
        "print(my_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF_WDjK1uFY1",
        "outputId": "2e5255eb-e293-4135-b64a-c18f2418c583"
      },
      "id": "EF_WDjK1uFY1",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "<context>\n",
            "Act 5, Scene 3, Line 321 : PRINCE : Than this of Juliet and her Romeo.  \n",
            "Act 5, Scene 3, Line 313 : MONTAGUE : As that of true and faithful Juliet.\n",
            "Act 5, Scene 3, Line 86 : ROMEO : For here lies Juliet, and her beauty makes\n",
            "Act 5, Scene 3, Line 300 : PRINCE : Came to this vault to die, and lie with Juliet.  \n",
            "Act 5, Scene 3, Line 246 : FRIAR LAURENCE : For whom, and not for Tybalt, Juliet pined.\n",
            "</context>\n",
            "\n",
            "Answer: Astra belongs to the Montague family.\n",
            "\n",
            "In English: Juliet belongs to the Montague family.\n",
            "\n",
            "In Pig Latin: ulietJay elongsbay otay hetay ontagueMay amilyfay.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f31c2d97",
      "metadata": {
        "id": "f31c2d97"
      },
      "source": [
        "## Additional resources\n",
        "\n",
        "To learn more about Amazon Bedrock, visit this page: [Introduction to Amazon Bedrock](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/introduction-to-bedrock)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}